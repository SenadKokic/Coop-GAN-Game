{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same DCGAN premise as prior - however, changing the \"game\" played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import statistics\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"/Users/senadkokic/Desktop/F2023/STAT940/Final Project/data\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 20\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "# plot training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on ``netG`` and ``netD``\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf) x 32 x 32``\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. ``(nc) x 64 x 64``\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-GPU\n",
    "if (device.type == 'mps') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly initialize weights\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # Define individual layers\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(ndf*2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(ndf*4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Apply layers and capture intermediate outputs\n",
    "        output1 = self.layer1(input)\n",
    "        output2 = self.layer2(output1)\n",
    "        output3 = self.layer3(output2)\n",
    "        output4 = self.layer4(output3)\n",
    "        final_output = self.final_layer(output4)\n",
    "\n",
    "        # Return final output and one or more intermediate outputs\n",
    "        return final_output,output1, output2, output3, output4  # we can choose which layers to return\n",
    "\n",
    "# adding the feedback loss layer    \n",
    "def feedback_loss_layer(disc_intermediate_output_fake, disc_intermediate_output_real):\n",
    "    # Calculate the L1 loss between the intermediate outputs\n",
    "    return F.l1_loss(disc_intermediate_output_fake, disc_intermediate_output_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-GPU\n",
    "if (device.type == 'mps') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly initialize weights\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ``BCELoss`` function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up for anomaly detection (for debugging purposes)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "# lambdas for strength of mechanism\n",
    "lambda_fb = 0.6\n",
    "lambda_reward = 0.25\n",
    "lambda_punish = 0.1\n",
    "\n",
    "# thresholds for feedback mechanism\n",
    "epoch_threshold_1 = 3\n",
    "epoch_threshold_2 = 10\n",
    "epoch_threshold_3 = 17\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_data, _) in enumerate(dataloader):\n",
    "        ############################\n",
    "        # Update D network\n",
    "        ###########################\n",
    "        netD.zero_grad()\n",
    "        real_cpu = real_data.to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "\n",
    "        \n",
    "        # Phase 1: Punish\n",
    "        if (epoch+1) % 4 != 0:\n",
    "            # Process real data\n",
    "            label_real = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "            output_real,_,_,_,_ = netD(real_cpu)\n",
    "            output_real = output_real.view(-1)  # Flatten the output for loss calculation\n",
    "            errD_real = criterion(output_real, label_real)\n",
    "            D_x = output_real.mean().item()\n",
    "\n",
    "            # Process fake data\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            label_fake = torch.full((b_size,), fake_label, dtype=torch.float, device=device)\n",
    "            output_fake,_,_,_,_ = netD(fake.detach())\n",
    "            output_fake = output_fake.view(-1)  # Flatten the output for loss calculation\n",
    "            errD_fake = criterion(output_fake, label_fake)\n",
    "            D_G_z1 = output_fake.mean().item()\n",
    "\n",
    "            # Apply punishment\n",
    "            punish = lambda_punish * torch.mean(((output_fake > 0.5).float() == label_fake).float())\n",
    "            errD = errD_real + errD_fake + punish\n",
    "\n",
    "        # Phase 2: Reward \n",
    "        else:\n",
    "            # Process a subset of real data\n",
    "            split_index = int(b_size * 0.75)  # Adjust the split index as needed\n",
    "            real_data_subset = real_cpu[:split_index]\n",
    "            unused_real_data = real_cpu[split_index:]\n",
    "            label_subset = torch.full((split_index,), real_label, dtype=torch.float, device=device)\n",
    "\n",
    "            # Forward pass and compute loss for the subset of real data\n",
    "            output_subset,_,_,_,_ = netD(real_data_subset)\n",
    "            output_subset = output_subset.view(-1)\n",
    "            errD_real_subset = criterion(output_subset, label_subset)\n",
    "            D_x = output_subset.mean().item()\n",
    "\n",
    "            # Generate fake data\n",
    "            noise = torch.randn(b_size - split_index, nz, 1, 1, device=device)\n",
    "            fake_data = netG(noise)\n",
    "            fake_label_subset = torch.full((b_size - split_index,), fake_label, dtype=torch.float, device=device)\n",
    "\n",
    "            # Combine and shuffle unused real data with fake data\n",
    "            combined_data = torch.cat((unused_real_data, fake_data), dim=0)\n",
    "            combined_labels = torch.cat((torch.ones(unused_real_data.size(0), device=device), torch.zeros(fake_data.size(0), device=device)), dim=0)\n",
    "            indices = torch.randperm(combined_data.size(0))\n",
    "            combined_data = combined_data[indices]\n",
    "            combined_labels = combined_labels[indices]\n",
    "\n",
    "            # Forward pass and compute loss for combined data\n",
    "            output_combined,_,_,_,_ = netD(combined_data)\n",
    "            output_combined = output_combined.view(-1) \n",
    "            errD_combined = criterion(output_combined, combined_labels)\n",
    "            D_G_z1 = output_combined.mean().item()\n",
    "\n",
    "            # Apply reward\n",
    "            reward = lambda_reward * torch.mean(((output_combined > 0.5).float() == combined_labels).float())\n",
    "            errD = errD_real_subset + errD_combined - reward\n",
    "\n",
    "        # Update D\n",
    "        errD.backward()\n",
    "        optimizerD.step()        \n",
    "\n",
    "\n",
    "        ############################\n",
    "        # Update G network\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        # Generate batch of latent vectors and fake images\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "\n",
    "        # Create label tensor filled with real_label\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, interm_output1, interm_output2, interm_output3, interm_output4 = netD(fake)\n",
    "        output = output.view(-1)  # Flatten the output for loss calculation\n",
    "        \n",
    "        # Forward pass real batch through D to get real intermediate outputs\n",
    "        _, interm_output_real1, interm_output_real2, interm_output_real3, interm_output_real4 = netD(real_cpu)\n",
    "\n",
    "        # Select feedback loss based on current epoch\n",
    "        if (epoch+1) < epoch_threshold_1:\n",
    "            fb_loss = feedback_loss_layer(interm_output1, interm_output_real1)\n",
    "        elif (epoch+1) < epoch_threshold_2:\n",
    "            fb_loss = feedback_loss_layer(interm_output2, interm_output_real2)\n",
    "        elif (epoch+1) < epoch_threshold_3:\n",
    "            fb_loss = feedback_loss_layer(interm_output3, interm_output_real3)\n",
    "        else:\n",
    "            fb_loss = feedback_loss_layer(interm_output4, interm_output_real4)\n",
    "\n",
    "        # Calculate traditional GAN loss\n",
    "        errG = criterion(output, label)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_errG = errG + lambda_fb * fb_loss \n",
    "\n",
    "        # Calculate gradients for G and update G\n",
    "        D_G_z2 = output.mean().item()\n",
    "        total_errG.backward()\n",
    "        optimizerG.step()        \n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 100 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f\\tfb_loss: %.4f'\n",
    "                  % (epoch+1, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), total_errG.item(), D_x, D_G_z1, D_G_z2,fb_loss))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(total_errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\",linewidth=0.2)\n",
    "plt.plot(D_losses,label=\"D\",linewidth=0.2)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(G_losses))\n",
    "print(statistics.mean(D_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
